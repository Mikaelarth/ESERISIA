"""
ESERISIA AI - DISTRIBUTED TRAINING SYSTEM
========================================
Syst√®me d'entra√Ænement distribu√© ultra-avanc√© pour ESERISIA AI
Support multi-GPU, multi-n≈ìud avec optimisations quantiques
"""

import torch
import torch.nn as nn
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from typing import Dict, List, Optional, Any, Callable
import numpy as np
import asyncio
import json
from datetime import datetime
from dataclasses import dataclass
import os
import logging

# Configuration du logging distribu√©
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingMetrics:
    """M√©triques d'entra√Ænement distribu√©"""
    epoch: int
    global_step: int
    loss: float
    accuracy: float
    learning_rate: float
    throughput: float  # samples/sec
    gpu_utilization: float
    memory_usage: float
    gradient_norm: float
    training_time: float

@dataclass
class DistributedConfig:
    """Configuration de l'entra√Ænement distribu√©"""
    world_size: int = 1
    rank: int = 0
    local_rank: int = 0
    backend: str = 'nccl'
    init_method: str = 'env://'
    master_addr: str = 'localhost'
    master_port: int = 12355
    
class EserisiaDistributedTrainer:
    """
    Syst√®me d'entra√Ænement distribu√© ESERISIA AI
    
    Fonctionnalit√©s:
    - Entra√Ænement multi-GPU haute performance
    - Synchronisation des gradients optimis√©e
    - Support multi-n≈ìud avec d√©couverte automatique
    - Int√©gration avec le syst√®me quantique
    - Monitoring en temps r√©el
    """
    
    def __init__(self, 
                 model: nn.Module,
                 config: DistributedConfig,
                 quantum_integration: bool = True):
        """
        Initialise le syst√®me d'entra√Ænement distribu√©
        
        Args:
            model: Mod√®le √† entra√Æner
            config: Configuration distribu√©e
            quantum_integration: Utiliser l'optimisation quantique
        """
        print("üöÄ Initialisation ESERISIA Distributed Training System...")
        
        self.model = model
        self.config = config
        self.quantum_integration = quantum_integration
        self.version = "DISTRIBUTED-2.0.0"
        
        # √âtat de l'entra√Ænement
        self.is_initialized = False
        self.is_training = False
        self.current_epoch = 0
        self.global_step = 0
        
        # M√©triques
        self.training_history = []
        self.best_metrics = None
        
        # Configuration GPU
        self.device = f'cuda:{config.local_rank}' if torch.cuda.is_available() else 'cpu'
        self.world_size = config.world_size
        self.rank = config.rank
        
        # Initialisation
        self._setup_distributed_environment()
        self._initialize_model()
        
        if quantum_integration:
            self._initialize_quantum_integration()
        
        print(f"‚úÖ ESERISIA Distributed Trainer v{self.version} initialis√©")
        print(f"üåê Configuration: Rank {self.rank}/{self.world_size}, Device: {self.device}")
    
    def _setup_distributed_environment(self):
        """Configure l'environnement distribu√©"""
        if self.world_size > 1:
            try:
                # Initialisation du processus distribu√©
                if not dist.is_initialized():
                    dist.init_process_group(
                        backend=self.config.backend,
                        init_method=self.config.init_method,
                        world_size=self.world_size,
                        rank=self.rank
                    )
                
                # Configuration du device local
                if torch.cuda.is_available():
                    torch.cuda.set_device(self.config.local_rank)
                    self.device = f'cuda:{self.config.local_rank}'
                
                logger.info(f"Processus distribu√© initialis√©: rank {self.rank}/{self.world_size}")
                
            except Exception as e:
                logger.warning(f"Erreur initialisation distribu√©e: {e}")
                self.world_size = 1  # Fallback mode single GPU
        
        self.is_distributed = self.world_size > 1 and dist.is_initialized()
    
    def _initialize_model(self):
        """Initialise le mod√®le pour l'entra√Ænement distribu√©"""
        # D√©placement du mod√®le sur le device appropri√©
        self.model = self.model.to(self.device)
        
        # Enveloppement DDP si distribu√©
        if self.is_distributed:
            self.model = DDP(
                self.model,
                device_ids=[self.config.local_rank] if torch.cuda.is_available() else None,
                output_device=self.config.local_rank if torch.cuda.is_available() else None,
                find_unused_parameters=True
            )
            logger.info("Mod√®le encapsul√© dans DistributedDataParallel")
        
        self.is_initialized = True
    
    def _initialize_quantum_integration(self):
        """Initialise l'int√©gration avec le syst√®me quantique"""
        try:
            from ..quantum.quantum_core import eserisia_quantum
            self.quantum_core = eserisia_quantum
            
            if self.quantum_core is not None:
                logger.info("‚úÖ Int√©gration quantique activ√©e")
            else:
                logger.warning("‚ö†Ô∏è Quantum core non disponible")
                self.quantum_integration = False
                
        except ImportError:
            logger.warning("‚ö†Ô∏è Module quantique non trouv√©")
            self.quantum_integration = False
    
    async def train_epoch(self, 
                         dataloader: DataLoader,
                         optimizer: torch.optim.Optimizer,
                         criterion: nn.Module,
                         scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
                         gradient_clipping: Optional[float] = None) -> TrainingMetrics:
        """
        Entra√Æne le mod√®le pour une √©poque
        
        Args:
            dataloader: DataLoader pour les donn√©es d'entra√Ænement
            optimizer: Optimiseur
            criterion: Fonction de perte
            scheduler: Planificateur de taux d'apprentissage (optionnel)
            gradient_clipping: Seuil de coupure des gradients (optionnel)
            
        Returns:
            TrainingMetrics pour cette √©poque
        """
        if not self.is_initialized:
            raise RuntimeError("Trainer non initialis√©")
        
        self.model.train()
        self.is_training = True
        
        epoch_start = datetime.now()
        total_loss = 0.0
        total_samples = 0
        correct_predictions = 0
        
        # Configuration du sampler distribu√©
        if hasattr(dataloader.sampler, 'set_epoch'):
            dataloader.sampler.set_epoch(self.current_epoch)
        
        for batch_idx, (data, targets) in enumerate(dataloader):
            batch_start = datetime.now()
            
            # D√©placement des donn√©es
            data = data.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = self.model(data)
            loss = criterion(outputs, targets)
            
            # Backward pass
            loss.backward()
            
            # Optimisation quantique des gradients si activ√©e
            if self.quantum_integration and self.quantum_core is not None:
                await self._quantum_gradient_optimization(optimizer)
            
            # Gradient clipping
            if gradient_clipping is not None:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clipping)
            
            # Optimisation
            optimizer.step()
            
            # M√©triques
            batch_time = (datetime.now() - batch_start).total_seconds()
            batch_size = data.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size
            
            # Calcul de l'accuracy (si classification)
            if outputs.dim() > 1 and outputs.size(1) > 1:
                predictions = torch.argmax(outputs, dim=1)
                correct_predictions += (predictions == targets).sum().item()
            
            # Logging p√©riodique
            if batch_idx % 100 == 0:
                avg_loss = total_loss / total_samples
                throughput = batch_size / batch_time
                
                logger.info(f"√âpoque {self.current_epoch}, Batch {batch_idx}: "
                          f"Loss={avg_loss:.4f}, Throughput={throughput:.1f} samples/s")
            
            self.global_step += 1
        
        # Synchronisation distribu√©e des m√©triques
        epoch_metrics = await self._gather_epoch_metrics(
            total_loss, total_samples, correct_predictions,
            (datetime.now() - epoch_start).total_seconds()
        )
        
        # Mise √† jour du scheduler
        if scheduler is not None:
            scheduler.step()
        
        # Sauvegarde p√©riodique
        if self.current_epoch % 10 == 0:
            await self._save_checkpoint()
        
        self.current_epoch += 1
        self.training_history.append(epoch_metrics)
        
        return epoch_metrics
    
    async def _quantum_gradient_optimization(self, optimizer: torch.optim.Optimizer):
        """Optimise les gradients avec le syst√®me quantique"""
        if not self.quantum_integration:
            return
        
        try:
            # Collecte des gradients
            gradients = []
            for param in self.model.parameters():
                if param.grad is not None:
                    gradients.append(param.grad.data.flatten())
            
            if not gradients:
                return
            
            # Concat√©nation des gradients
            all_gradients = torch.cat(gradients)
            
            # Optimisation quantique
            optimized_gradients = await self.quantum_core.quantum_neural_optimization(
                all_gradients.unsqueeze(0)
            )
            
            # Redistribution des gradients optimis√©s
            optimized_flat = optimized_gradients.classical_output.flatten()
            idx = 0
            
            for param in self.model.parameters():
                if param.grad is not None:
                    grad_size = param.grad.data.numel()
                    param.grad.data = optimized_flat[idx:idx + grad_size].view_as(param.grad.data)
                    idx += grad_size
            
        except Exception as e:
            logger.warning(f"Erreur optimisation quantique des gradients: {e}")
    
    async def _gather_epoch_metrics(self, 
                                   total_loss: float,
                                   total_samples: int,
                                   correct_predictions: int,
                                   epoch_time: float) -> TrainingMetrics:
        """Collecte et synchronise les m√©triques de l'√©poque"""
        
        # Moyennes locales
        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0
        throughput = total_samples / epoch_time if epoch_time > 0 else 0.0
        
        # Synchronisation distribu√©e
        if self.is_distributed:
            # Cr√©ation de tensors pour la synchronisation
            metrics_tensor = torch.tensor([
                avg_loss, accuracy, throughput, float(total_samples)
            ], device=self.device)
            
            # All-reduce pour moyenner sur tous les processus
            dist.all_reduce(metrics_tensor, op=dist.ReduceOp.SUM)
            metrics_tensor /= self.world_size
            
            avg_loss, accuracy, throughput, _ = metrics_tensor.tolist()
        
        # M√©triques syst√®me
        gpu_util = self._get_gpu_utilization()
        memory_usage = self._get_memory_usage()
        gradient_norm = self._calculate_gradient_norm()
        
        return TrainingMetrics(
            epoch=self.current_epoch,
            global_step=self.global_step,
            loss=avg_loss,
            accuracy=accuracy,
            learning_rate=self._get_current_lr(),
            throughput=throughput,
            gpu_utilization=gpu_util,
            memory_usage=memory_usage,
            gradient_norm=gradient_norm,
            training_time=epoch_time
        )
    
    def _get_gpu_utilization(self) -> float:
        """Obtient l'utilisation GPU"""
        if not torch.cuda.is_available():
            return 0.0
        
        try:
            return torch.cuda.utilization(self.config.local_rank)
        except Exception:
            return 0.0
    
    def _get_memory_usage(self) -> float:
        """Obtient l'utilisation m√©moire GPU en MB"""
        if not torch.cuda.is_available():
            return 0.0
        
        try:
            return torch.cuda.memory_allocated(self.config.local_rank) / 1024**2
        except Exception:
            return 0.0
    
    def _calculate_gradient_norm(self) -> float:
        """Calcule la norme des gradients"""
        total_norm = 0.0
        param_count = 0
        
        for param in self.model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        return (total_norm ** (1. / 2)) if param_count > 0 else 0.0
    
    def _get_current_lr(self) -> float:
        """Obtient le taux d'apprentissage actuel"""
        # Retourne le dernier LR utilis√© ou une valeur par d√©faut
        return 0.001  # Placeholder - devrait √™tre extrait de l'optimizer
    
    async def _save_checkpoint(self):
        """Sauvegarde un checkpoint de l'entra√Ænement"""
        if self.rank != 0:  # Seulement le processus ma√Ætre sauvegarde
            return
        
        try:
            checkpoint = {
                'epoch': self.current_epoch,
                'global_step': self.global_step,
                'model_state_dict': self.model.module.state_dict() if self.is_distributed else self.model.state_dict(),
                'training_history': self.training_history,
                'config': self.config,
                'version': self.version
            }
            
            checkpoint_path = f"checkpoint_distributed_epoch_{self.current_epoch}.pt"
            torch.save(checkpoint, checkpoint_path)
            
            logger.info(f"Checkpoint sauvegard√©: {checkpoint_path}")
            
        except Exception as e:
            logger.error(f"Erreur sauvegarde checkpoint: {e}")
    
    async def validate(self, 
                      val_dataloader: DataLoader,
                      criterion: nn.Module) -> Dict[str, float]:
        """
        Validation du mod√®le
        
        Args:
            val_dataloader: DataLoader de validation
            criterion: Fonction de perte
            
        Returns:
            Dictionnaire des m√©triques de validation
        """
        self.model.eval()
        
        total_val_loss = 0.0
        total_val_samples = 0
        correct_val_predictions = 0
        
        with torch.no_grad():
            for data, targets in val_dataloader:
                data = data.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
                
                outputs = self.model(data)
                loss = criterion(outputs, targets)
                
                batch_size = data.size(0)
                total_val_loss += loss.item() * batch_size
                total_val_samples += batch_size
                
                if outputs.dim() > 1 and outputs.size(1) > 1:
                    predictions = torch.argmax(outputs, dim=1)
                    correct_val_predictions += (predictions == targets).sum().item()
        
        # Synchronisation distribu√©e des m√©triques de validation
        if self.is_distributed:
            val_metrics_tensor = torch.tensor([
                total_val_loss, float(total_val_samples), float(correct_val_predictions)
            ], device=self.device)
            
            dist.all_reduce(val_metrics_tensor, op=dist.ReduceOp.SUM)
            total_val_loss, total_val_samples, correct_val_predictions = val_metrics_tensor.tolist()
        
        val_metrics = {
            'val_loss': total_val_loss / total_val_samples if total_val_samples > 0 else 0.0,
            'val_accuracy': correct_val_predictions / total_val_samples if total_val_samples > 0 else 0.0
        }
        
        return val_metrics
    
    def get_training_status(self) -> Dict[str, Any]:
        """Retourne le statut de l'entra√Ænement distribu√©"""
        return {
            "distributed_trainer": "ESERISIA Distributed Training System",
            "version": self.version,
            "status": "üü¢ TRAINING" if self.is_training else "üü° READY",
            "configuration": {
                "world_size": self.world_size,
                "rank": self.rank,
                "device": str(self.device),
                "distributed": self.is_distributed,
                "quantum_integration": self.quantum_integration
            },
            "progress": {
                "current_epoch": self.current_epoch,
                "global_step": self.global_step,
                "total_epochs_completed": len(self.training_history)
            },
            "performance": {
                "best_metrics": self.best_metrics,
                "recent_throughput": self.training_history[-1].throughput if self.training_history else 0.0,
                "avg_gpu_utilization": np.mean([m.gpu_utilization for m in self.training_history]) if self.training_history else 0.0,
                "avg_memory_usage": np.mean([m.memory_usage for m in self.training_history]) if self.training_history else 0.0
            },
            "capabilities": [
                "üåê Entra√Ænement Multi-GPU Haute Performance",
                "üîó Synchronisation de Gradients Optimis√©e",
                "‚öõÔ∏è Int√©gration Optimisation Quantique",
                "üìä Monitoring Temps R√©el",
                "üíæ Checkpoints Automatiques",
                "üéØ Validation Distribu√©e",
                "‚ö° Throughput Ultra-Rapide",
                "üß† Compatible avec tous Mod√®les PyTorch"
            ],
            "description": "Syst√®me d'entra√Ænement distribu√© le plus avanc√© pour ESERISIA AI"
        }
    
    def cleanup(self):
        """Nettoie les ressources distribu√©es"""
        if self.is_distributed and dist.is_initialized():
            dist.destroy_process_group()
            logger.info("Processus distribu√© nettoy√©")

# Fonction utilitaire pour lancer l'entra√Ænement distribu√©
def launch_distributed_training(
    model_fn: Callable,
    train_data_fn: Callable,
    config: DistributedConfig,
    **training_kwargs
):
    """
    Lance l'entra√Ænement distribu√©
    
    Args:
        model_fn: Fonction qui retourne le mod√®le √† entra√Æner
        train_data_fn: Fonction qui retourne le DataLoader d'entra√Ænement
        config: Configuration distribu√©e
        **training_kwargs: Arguments suppl√©mentaires pour l'entra√Ænement
    """
    def training_process(rank: int, world_size: int):
        # Configuration du processus
        config.rank = rank
        config.world_size = world_size
        config.local_rank = rank % torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        # Cr√©ation du mod√®le et des donn√©es
        model = model_fn()
        train_loader = train_data_fn()
        
        # Cr√©ation du trainer
        trainer = EserisiaDistributedTrainer(model, config)
        
        # Lancement de l'entra√Ænement
        asyncio.run(train_distributed(trainer, train_loader, **training_kwargs))
        
        # Nettoyage
        trainer.cleanup()
    
    # Lancement multi-processus
    if config.world_size > 1:
        mp.spawn(training_process, args=(config.world_size,), nprocs=config.world_size)
    else:
        training_process(0, 1)

async def train_distributed(trainer: EserisiaDistributedTrainer,
                           train_loader: DataLoader,
                           epochs: int = 100,
                           **kwargs):
    """Fonction d'entra√Ænement distribu√© compl√®te"""
    
    # Configuration de l'optimiseur et autres composants
    optimizer = torch.optim.AdamW(trainer.model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    logger.info(f"D√©but de l'entra√Ænement distribu√© pour {epochs} √©poques")
    
    for epoch in range(epochs):
        # Entra√Ænement d'une √©poque
        metrics = await trainer.train_epoch(
            train_loader, optimizer, criterion, scheduler, gradient_clipping=1.0
        )
        
        if trainer.rank == 0:  # Logging seulement du processus ma√Ætre
            logger.info(f"√âpoque {epoch}: Loss={metrics.loss:.4f}, "
                       f"Accuracy={metrics.accuracy:.4f}, "
                       f"Throughput={metrics.throughput:.1f} samples/s")

if __name__ == "__main__":
    print("üöÄ ESERISIA Distributed Training System - Ready for deployment")
